{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d32eee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Downloading vgg_lpips model from https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1 to taming/modules/autoencoder/lpips/vgg.pth\n",
      "8.19kB [00:00, 362kB/s]                                                         \n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "  0%|                    | 1/390 [00:06<20:25,  3.15s/it, Transformer_Loss=9.48]^C\n"
     ]
    }
   ],
   "source": [
    "!python ./MaskGIT-pytorch/training_transformer.py --batch-size=10 --num-codebook-vectors=16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66f4585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "100%|█████████████████| 130/130 [07:58<00:00,  3.68s/it, Transformer_Loss=0.728]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Figure(640x480)\n",
      "100%|██████████████████| 130/130 [07:59<00:00,  3.69s/it, Transformer_Loss=1.19]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Figure(640x480)\n",
      "100%|███████████████████| 130/130 [08:00<00:00,  3.69s/it, Transformer_Loss=5.2]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Figure(640x480)\n",
      "100%|██████████████████| 130/130 [07:59<00:00,  3.69s/it, Transformer_Loss=5.92]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Figure(640x480)\n",
      "100%|█████████████████| 130/130 [07:59<00:00,  3.69s/it, Transformer_Loss=0.117]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python ./MaskGIT-pytorch/training_transformer.py --batch-size=10 --num-codebook-vectors=16384 --epochs=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38fa65c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 29 04:53:27 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P0    67W / 149W |      0MiB / 11441MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e14ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "  3%|▌                     | 10/390 [00:39<26:09,  4.13s/it, Transformer_Loss=7]^C\n",
      "  3%|▌                     | 10/390 [00:43<27:34,  4.35s/it, Transformer_Loss=7]\n",
      "Traceback (most recent call last):\n",
      "  File \"./MaskGIT-pytorch/training_transformer.py\", line 99, in <module>\n",
      "    train_transformer = TrainTransformer(args)\n",
      "  File \"./MaskGIT-pytorch/training_transformer.py\", line 21, in __init__\n",
      "    self.train(args)\n",
      "  File \"./MaskGIT-pytorch/training_transformer.py\", line 33, in train\n",
      "    self.optim.step()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/optim/adamw.py\", line 148, in step\n",
      "    eps=group['eps'])\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/optim/_functional.py\", line 139, in adamw\n",
      "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./MaskGIT-pytorch/training_transformer.py --batch-size=10 --num-codebook-vectors=16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e17230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.yaml\r\n"
     ]
    }
   ],
   "source": [
    "ls ./taming-transformers/logs/vqgan_imagenet_f16_16384/configs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc6492d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config_path = r\"./taming-transformers/logs/vqgan_imagenet_f16_16384/configs/model.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "config.model.params['n_embed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils as vutils\n",
    "from transformer import VQGANTransformer\n",
    "from utils import load_data, plot_images\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ygong2832/taming-transformers')\n",
    "sys.path.insert(0, '/home/ygong2832/taming-transformers/taming')\n",
    "# import model\n",
    "model = VQGANTransformer(args).to(device=args.device)\n",
    "model.load_state_dict(torch.load('path'))\n",
    "model.eval()\n",
    "# test\n",
    "with tqdm(range(len(test_dataset))) as pbar:\n",
    "    for i, imgs in zip(pbar, test_dataset):\n",
    "        real = imgs.to(device=args.device)\n",
    "        vutils.save_image(real, os.path.join(\"real\", f\"{i}.jpg\"))\n",
    "        logits, target = model(real)\n",
    "        vutils.save_image(real, os.path.join(\"fake\", f\"{i}.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
